{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The green channel at (12,13) is 0.368627\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import datasets, utils\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train = utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test  = utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# get the green channel of pixel (12,13)\n",
    "print(\"The green channel at (12,13) is %f\" % x_train[54,12,13,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Sequentail build\n",
    "The following Builds the MLP (sequential layers of the NN)\n",
    "```\n",
    "Sequential method of building the MLP\n",
    "\n",
    "model = models.Sequential( [\n",
    "    layers.Flatten(input_shape=(32,32,3)),\n",
    "    layers.Dense(200, activation = 'relu'),\n",
    "    layers.Dense(150, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax'),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Building the MLP via the functional API\n",
    "\"\"\"\n",
    "input_layer = layers.Input(shape=(32,32,3))\n",
    "x = layers.Flatten()(input_layer)\n",
    "x = layers.Dense(units=200, activation = 'relu')(x)\n",
    "x = layers.Dense(units=150, activation = 'relu')(x)\n",
    "output_layer = layers.Dense(units=10, activation = 'softmax')(x)\n",
    "model = models.Model(input_layer, output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Activation function\n",
    "The 'relu' and 'softmax' are activation functions, or\n",
    " more accurately, a description of them. ReLU is\n",
    " \"Rectified Linear Unit\"\n",
    " More information can be found here:\n",
    "\n",
    " https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layers\n",
    "\n",
    "There are three different types of layers here, **Input**, **Flatten**, and **Dense**. \n",
    "\n",
    "The **Input** layer is the entry layer of course. The important part here is to specify the size of *each* input, as opposed to the number of inputs. This can then be *flattened* using a **Flatten** layer, resulting in a vector 32 * 32 * 3 = 3,072 elements long. \n",
    "\n",
    "The layer has to be flatten because a **Dense** layer requires its input to be flat, rather than multidimensional.\n",
    "\n",
    "The **Dense** layer is one of the fundamental building blocks of a NN. It serves as the hidden layer that is doing the bulk of connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               614600    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1510      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 646260 (2.47 MB)\n",
      "Trainable params: 646260 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "opt = optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "The **loss** specifier above is a way to specify how to compute the error. \n",
    "\n",
    "A few types are:\n",
    "* For continuous output, a *mean squared error* loss is usually a good choice.\n",
    "* For classification problems, where an observation can only belong to a single class, a *categorical cross-entropy* loss fucntion is a good choice.\n",
    "* If the the output could be labelled several ways, i.e. could belong to several classes, then a *binary cross-entropy* loss function is a good choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "Optimizers are the algorithms used to udpate the weight in the NN, based on the gradident of the loss function. \"Adam\" is a very common optimizer, and stands for Adaptive Moment Estimation. \n",
    "\n",
    "Another common optimizer is the RMSProp, or Root Mean Squared Propagation. More information can be found with the KERAS documentation:\n",
    "\n",
    "https://keras.io/api/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of learning\n",
    "\n",
    "### Supervised Learning\n",
    "This is the m x n matrix M, that is fed into a algorithm to create a map, from the inputs to the outputs and largely used to discriminate the inputs into labels or categories. The two classes of supervised learning are:\n",
    "* Regression, for continuous classifications; for example predicting house prices at a location.\n",
    "* Classification, for discrete and bounded targets.\n",
    "\n",
    "### Unsupervised Learning\n",
    "The goal here is to learn a representation of the data that best describes it. There is no labeled data (inputs) and the goal is to draw out some unknown pattern from the raw data. \n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "This consists of an agent learning by interacting with an environment. The agent takes an action is either rewarded or punished by that action, and moves on to another state. The overall goal is to find a policy or mapping function that maximizes the rewards and minimizes the punishments. Examples are in games, such as chess, and Go, or path-finding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8498 - accuracy: 0.3345\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.6678 - accuracy: 0.4044\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5870 - accuracy: 0.4330\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.5229 - accuracy: 0.4567\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4859 - accuracy: 0.4700\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4519 - accuracy: 0.4817\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4245 - accuracy: 0.4931\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4040 - accuracy: 0.5004\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3816 - accuracy: 0.5064\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3592 - accuracy: 0.5176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28fefa730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training the Model\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size= 32,\n",
    "          epochs = 10,\n",
    "          shuffle = True)\n",
    "\n",
    "# x_train is the raw image data\n",
    "# y_train the one-hot encoded class labels\n",
    "# batch_size determines the number of observations passed to the network \n",
    "#   at each step, in this case, how many images are in each training step.\n",
    "#   The larger the batch size the more stable the gradient calculation, but \n",
    "#   this slows down the training. Generally, a batch size between 32 and 256\n",
    "#   is used. \n",
    "# epochs is the number of times the network will be shown the full training data.\n",
    "# shuffle indicates that the batches will be drawn randomly without reaplcement \n",
    "#   from the training data at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 740us/step - loss: 1.4396 - accuracy: 0.4859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4396058320999146, 0.48590001463890076]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here is a list of the metrics we're interested in, namely:\n",
    "\n",
    "* categorical cross-entropy \n",
    "* accuracy\n",
    "\n",
    "So, here, the accuracy is ~49%. \n",
    "\n",
    "For some more information on some of these terms, especially the cross categorical entropy, visit:\n",
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "\n",
    "Another is:\n",
    "https://neuralthreads.medium.com/categorical-cross-entropy-loss-the-most-important-loss-function-d3792151d05b\n",
    "\n",
    "For me, a working definition of the cross categorical entropy is the ability to discriminate categories. The larger the number, the worse the model's ability to discern the category, as seen in the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better prediction yields: 0.162519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Cross Entropy calculation\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "def cross_E(yt, yp):\n",
    "    return -np.sum(yt * np.log(yp + 10**-100))\n",
    "y_true = np.array([[0],[1],[0],[0]])\n",
    "y_pred = np.array([[0.05], [0.85], [0.10], [0]])\n",
    "\n",
    "high = cross_E(y_true, y_pred)\n",
    "print(\"Better prediction yields: %f\" % high)\n",
    "# 0.16251892949777494\n",
    "y_pred = np.array([[0.1],[0.5],[0.4],[0]]) # make a weaker prediction\n",
    "lo   = cross_E(y_true, y_pred)\n",
    "# 0.6931471805599453\n",
    "print(\"Worse prediction yields: %f\" % lo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
