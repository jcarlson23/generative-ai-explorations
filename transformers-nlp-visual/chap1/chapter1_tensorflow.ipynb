{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EV9Sas9vFRdV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same thing, but this time we'll use the TPU"
      ],
      "metadata": {
        "id": "fGTu_aj1FhO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 512\n",
        "d = 512\n",
        "\n",
        "input_seq = tf.random.normal((n,d), dtype=tf.float32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0i37PkzZFh5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the matrix multiplication"
      ],
      "metadata": {
        "id": "W_wWaDjDFsYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "_ = tf.matmul(input_seq, input_seq, transpose_b=True)\n",
        "at = time.time() - start_time\n",
        "print(f\"Self-attention computation time: {at} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO51Q7koFwIG",
        "outputId": "8c3ec51c-9280-41ad-c7bf-d33309c0e7a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-attention computation time: 0.029388427734375 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time for the recurrent layer"
      ],
      "metadata": {
        "id": "JgQY9VokGPEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "hidden_state = np.zeros((n,d), dtype=np.float32)\n",
        "for i in range(n):\n",
        "  for j in range(d):\n",
        "    for k in range(d):\n",
        "      hidden_state[i,j] += input_seq[i,j].numpy() * hidden_state[min(i,k),j]\n",
        "      ct = time.time() - start_time\n",
        "      if ct > at*10:\n",
        "        break\n",
        "\n",
        "rt = time.time() - start_time\n",
        "print(f\"Recurrent layer computation time: {rt} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z4hI_oTGRIB",
        "outputId": "2b6aaa93-7689-492b-8f92-2bf1241bd54c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recurrent layer computation time: 74.95757699012756 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two..."
      ],
      "metadata": {
        "id": "xSQ-ldbyHwCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = at + rt\n",
        "\n",
        "percentage_at = round( (at/total) * 100, 2)\n",
        "\n",
        "print(f\"The percentage of self-attention computation is the sum of self-attention and recurrence, is: {percentage_at}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPn0ejstHxbm",
        "outputId": "789d388a-6798-4fea-a853-4d65e2794c48"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The percentage of self-attention computation is the sum of self-attention and recurrence, is: 0.04\n"
          ]
        }
      ]
    }
  ]
}