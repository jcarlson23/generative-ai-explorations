# -*- coding: utf-8 -*-
"""Transformers-Chapter2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DH-WB6oEcOrlBK5-5Ob1DDGhjyouBbVV
"""

from transformers import AutoTokenizer
model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

"""Let's grab the emotions data"""

!pip install datasets
from datasets import load_dataset
emotions = load_dataset("emotion")

import pandas as pd
import matplotlib.pyplot as plt

def tokenize(batch):
  return tokenizer(batch["text"], padding=True, truncation=True)

print(tokenize(emotions["train"][:2]))

emotions_encoded = emotions.map(tokenize,batched=True, batch_size=None)

print(emotions_encoded["train"].column_names)

"""Using a Transformer as a Feature Extractor"""

from transformers import AutoModel
import torch

# we already have the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModel.from_pretrained(model_ckpt).to(device)
print(f"We're using {device} as the device")

"""Extracing the (last) hidden state"""

text = "This is a test"
inputs = tokenizer(text, return_tensors="pt")
print(f"Input tensor shape: {inputs['input_ids'].size()}")

inputs = {k:v.to(device) for k,v in inputs.items()}
with torch.no_grad():
  outputs = model(**inputs)
print(outputs)

def extract_hidden_states(batch):
  inputs = {k:v.to(device) for k,v in batch.items()
            if k in tokenizer.model_input_names}
  # extract the state
  with torch.no_grad():
    last_hidden_state = model(**inputs).last_hidden_state
  return {"hidden_state": last_hidden_state[:,0].cpu().numpy()}

emotions_encoded.set_format("torch",columns=["input_ids","attention_mask","label"])

emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)
emotions_hidden["train"].column_names

"""Creating a feature matrix -- to extract features."""

import numpy as np
X_train = np.array(emotions_hidden["train"]["hidden_state"])
X_valid = np.array(emotions_hidden["validation"]["hidden_state"])
y_train = np.array(emotions_hidden["train"]["label"])
y_valid = np.array(emotions_hidden["validation"]["label"])
X_train.shape, X_valid.shape

!pip uninstall umap
!pip install umap-learn
from umap.umap_ import UMAP
from sklearn.preprocessing import MinMaxScaler

X_scaled = MinMaxScaler().fit_transform(X_train)
mapper = UMAP(n_components=2, metric="cosine").fit(X_scaled)
# create a dataframe
df_emb = pd.DataFrame(mapper.embedding_, columns=["X","Y"])
df_emb["label"] = y_train
df_emb.head()

"""Time to look at the various categories graphically."""

fig, axes = plt.subplots(2,3,figsize=(7,5))
axes = axes.flatten()
cmaps = ["Greys", "Blues", "Oranges", "Reds", "Purples", "Greens"]
labels = emotions["train"].features["label"].names

for i, (label,cmap) in enumerate(zip(labels, cmaps)):
  df_emb_sub = df_emb.query(f"label == {i}")
  axes[i].hexbin(df_emb_sub["X"],df_emb_sub["Y"],cmap=cmap, gridsize=20, linewidths=(0,))
  axes[i].set_title(label)
  axes[i].set_xticks([]), axes[i].set_yticks([])

plt.tight_layout()
plt.show()

"""Time to train a classifier on these hidden states"""

from sklearn.linear_model import LogisticRegression

# we increaese max_iter to ensure convergence
lr_clf = LogisticRegression(max_iter=3000)
lr_clf.fit(X_train,y_train)
lr_clf.score(X_valid,y_valid)

"""We should compare this against a baseline, in this case we can use the **DummyClassifier**."""

from sklearn.dummy import DummyClassifier
dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(X_train,y_train)
dummy_clf.score(X_valid,y_valid)

"""So this is actually, **significantly, better**."""