{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcarlson/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/d5/hdy_gn3j6y708g2rsdglvv_m0000gn/T/ipykernel_95204/3893147912.py:2: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  all_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133536 datasets currently available on the hub.\n",
      "The first 10 are ['acronym_identification', 'ade_corpus_v2', 'UCLNLP/adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'allenai/ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the hub.\")\n",
    "print(f\"The first 10 are {all_datasets[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcarlson/Library/Python/3.9/lib/python/site-packages/datasets/load.py:1461: FutureWarning: The repository for emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_ds[0])\n",
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to DataFrames\n",
    "\n",
    "Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.\n",
    "\n",
    "```d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n",
    "   col1  col2\n",
    "0     1     3\n",
    "1     2     4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand the label into a name - the integers aren't very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to look at the distribution ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have a maximum input sequence length that is called a maximum content size. For applications using DistilBERT, the max content size is 512 ttokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\",by=\"label_name\",grid=False, showfliers=False, color=\"black\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Time to take the strings (tweets) and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to encode for the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "categorical_df = pd.DataFrame(\n",
    "    {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.get_dummies(categorical_df[\"Name\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of this DataFrame are the one-hot vectors, which have a single \"hot\" entry with a 1 and 0s everywhere else. Now, looking at our input_ids, we have a similar problem: the elements create an ordinal scale. This means that adding or subtracting two IDs is a meaningless operation, since the result is a new ID that represents another random token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Vectors\n",
    "\n",
    "One hot encoding is a technique that we use to represent categorical variables as numerical values in a machine learning model.\n",
    "\n",
    "The advantages of using one hot encoding include:\n",
    "\n",
    "* It allows the use of categorical variables in models that require numerical input.\n",
    "* It can improve model performance by providing more information to the model about the categorical variable.\n",
    "* It can help to avoid the problem of ordinality, which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”).\n",
    "\n",
    "The disadvantages of using one hot encoding include:\n",
    "* It can lead to increased dimensionality, as a separate column is created for each category in the variable. This can make the model more complex and slow to train.\n",
    "* It can lead to sparse data, as most observations will have a value of 0 in most of the one-hot encoded columns.\n",
    "* It can lead to overfitting, especially if there are many categories in the variable and the sample size is relatively small.\n",
    "\n",
    "One-hot-encoding is a powerful technique to treat categorical data, but it can lead to increased dimensionality, sparsity, and overfitting. It is important to use it cautiously and consider other methods such as ordinal encoding or binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization\n",
    "\n",
    "Instead of splitting text into characters, words often makes more sense, and then those words are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one of the problems here is tokenization of the puncutation, which is very meaningful semantically. So, a compromise is **Subword Tokenization**, where the idea is to combine the best ideas of character tokenization and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings. On the other hand, we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageaable size. \n",
    "\n",
    "There are several subword tokenization algorithms commonly used in NLP, but here, we'll start with **WordPiece**, which is used by BERT and DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **AutoTokenizer** belongs to a larger set of \"auto\" classes, whose job is to automatically retrieve the model's configuration, pretrained weights, or vocabulary from the name of the checkpoint. This allows you to quickly switch between models, but if you wish to load the specific class manually you can do so as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer \n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text is a core task of NLP.\n",
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some special [CLS] and [SEP] tokens have been added to the start and end of the sequence. These tokens differ from model to model, but their main role is to indicate the start and end of a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n",
      "The vocabularly size 30522\n",
      "The model max length is 512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))\n",
    "print(f\"The vocabularly size {tokenizer.vocab_size}\")\n",
    "print(f\"The model max length is {tokenizer.model_max_length}\")\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first a procesing function \n",
    "def tokenize(batch):\n",
    "    return tokenizer(str(batch[\"text\"]), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              i didnt feel humiliated\n",
      "1    i can go from feeling so hopeless to so damned...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1014, 1045, 2134, 2102, 2514, 26608, 1015, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 1012, 1012, 1012, 2171, 1024, 3793, 1010, 26718, 18863, 1024, 4874, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(emotions[\"train\"][\"text\"][:2]))\n",
    "tokenize(emotions[\"train\"][:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This applies the processing function across all the splits of a corpus, in just a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Text Classifier\n",
    "\n",
    "We ahve two options:\n",
    "1. Use the hidden states of transformer and train a classifier on those hidden states; these does not need any modification of the pretrained model. \n",
    "2. Fine-tuning; where we train the whole model end-to-end.\n",
    "\n",
    "Let's try both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we using Cude? False\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "print(f\"Are we using Cude? {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple GPU is available? True\n"
     ]
    }
   ],
   "source": [
    "# if we can use the Apple GPU\n",
    "print(f\"Apple GPU is available? {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 6])\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Extract the hidden state for a single string\n",
    "text = \"this is a test\" \n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tesnor, to get the hidden state place the encodings on the same device as the model and pass the inputs (essentially, map it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are <class 'dict'>\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "print(f\"inputs are {type(inputs)}\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the same for the entire batch, we define a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the input_ids and attention_mask columns to torch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f80221bd59344539f8967dfae2a520f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (151) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m emotions_encoded\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# now we can extrac the hidden states across all splits in one go.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m emotions_hidden \u001b[38;5;241m=\u001b[39m \u001b[43memotions_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m, in \u001b[0;36mextract_hidden_states\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract last hidden states\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Return vector for [CLS] token\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: last_hidden_state[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:814\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    812\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 814\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    817\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:156\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    152\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (151) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
    "# now we can extrac the hidden states across all splits in one go.\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 0\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids \n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\":acc, \"f1\":f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login into Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7da51c8e0ce487aa6f30e5df710d9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcarlson/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jcarlson/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"])\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
